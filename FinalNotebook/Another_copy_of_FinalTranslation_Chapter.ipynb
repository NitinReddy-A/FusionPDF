{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitinReddy-A/FusionPDF/blob/main/Another_copy_of_FinalTranslation_Chapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D-kp0pLvR_e",
        "outputId": "346eb054-0211-4d1e-fd85-ca7d9369d959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (4.0.2)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.0.3)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->fitz) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (67.7.2)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.3)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.15.4)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.31.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2024.7.4)\n",
            "Installing collected packages: looseversion, traits, simplejson, isodate, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 prov-2.0.1 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.2 traits-6.3.2\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.6 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\n",
            "Collecting convertapi\n",
            "  Downloading convertapi-1.8.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: requests>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from convertapi) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (2024.7.4)\n",
            "Installing collected packages: convertapi\n",
            "Successfully installed convertapi-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fitz\n",
        "!pip install PyMuPDF\n",
        "!pip install convertapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7NHvi_NBouD"
      },
      "outputs": [],
      "source": [
        "# ! pip install --upgrade --force-reinstall --root-user-action ignore pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-TJ3Yu4vA9m"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import pymupdf\n",
        "import os\n",
        "import convertapi\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x35tpwxPwx98"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"/content/Page_1.pdf\"  #---------> Specify the input file\n",
        "doc = fitz.open(pdf_path)\n",
        "new_pdf_path = r\"/content/Translated_OurPast_Chapter1.pdf\" #-----------> Specify the output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MhzLvCz2InG"
      },
      "outputs": [],
      "source": [
        "new_doc = fitz.open()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6bxYsuz2Nw9"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/images/'):\n",
        "    os.mkdir('/content/images/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKpye6nU4Qkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd80ff02-a536-4652-a02a-256b8e9d90f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 0, 159, 217, 8, 'DeviceCMYK', '', 'Im0', 'JPXDecode', 0)\n",
            "Image 0 on page 0: Rect(445.9007873535156, 174.948974609375, 522.1007690429688, 278.64886474609375)\n"
          ]
        }
      ],
      "source": [
        "for page_index in range(doc.page_count):\n",
        "    page = doc.load_page(page_index)\n",
        "    # Get the original page\n",
        "    original_page = doc.load_page(page_index)\n",
        "\n",
        "    # Create a new page with the same size as the original page\n",
        "    new_page = new_doc.new_page(width=original_page.rect.width, height=original_page.rect.height)\n",
        "\n",
        "    # Get the list of images on the page\n",
        "    image_list = page.get_images(full=True)\n",
        "\n",
        "    # Iterate through each image and save it in the images folder\n",
        "    for img_index, img in enumerate(image_list):\n",
        "        try:\n",
        "            if img[1] == 0:\n",
        "                print(img)\n",
        "                bbox = page.get_image_bbox(img)\n",
        "                xref = img[0] # get the XREF of the image\n",
        "                pix = pymupdf.Pixmap(doc, xref) # create a Pixmap\n",
        "                if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
        "                    pix = pymupdf.Pixmap(pymupdf.csRGB, pix)\n",
        "                pix.save(f\"images/page{page_index}-image{img_index}.png\") # save the image as png\n",
        "                pix = None\n",
        "                new_page.insert_image(bbox, stream=open(f\"images/page{page_index}-image{img_index}.png\", \"rb\").read())\n",
        "                print(f\"Image {img_index} on page {page_index}: {bbox}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while extracting image: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_6LHLe54XTq"
      },
      "outputs": [],
      "source": [
        "new_doc.save(new_pdf_path)\n",
        "new_doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu5h-6bmiZhg",
        "outputId": "88fa5d85-ddfd-46af-97e0-8983a5f07865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 700 (delta 280), reused 341 (delta 238), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (700/700), 2.64 MiB | 13.44 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1404, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1404 (delta 139), reused 152 (delta 124), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1404/1404), 9.57 MiB | 12.18 MiB/s, done.\n",
            "Resolving deltas: 100% (749/749), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 28.71 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 600 (delta 10), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (600/600), 253.58 KiB | 7.25 MiB/s, done.\n",
            "Resolving deltas: 100% (359/359), done.\n",
            "/content\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sphinx>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=4.0.0->sphinx-argparse->indic-nlp-library) (2024.7.4)\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 portalocker-2.10.1 sacrebleu-2.4.2 sacremoses-0.1.1 sphinx-argparse-0.5.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.66.4)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49171 sha256=555e83490da59dfd3a1a9afd50136575dfa432cbb5ed87981ab08107757962d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=c6becfd9f5332faa852525f3e1987fe07a21b6b4a4cf4ef775d87bda37552161\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3336 sha256=0c6aa8c496fbb6e28d2d18df31b1029a1fee37a1ba3e9e538fa26d0f8128cbcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6146 sha256=f954dfc49934ea2d8cae1e73ad80cbfd2bc315940ea22d2dfa0cfc4f74312068\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35203, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 35203 (delta 66), reused 80 (delta 48), pack-reused 35083\u001b[K\n",
            "Receiving objects: 100% (35203/35203), 25.23 MiB | 20.20 MiB/s, done.\n",
            "Resolving deltas: 100% (25556/25556), done.\n",
            "/content/fairseq\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.25.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.5.15)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.4)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.1)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19926391 sha256=81c21a23464b173af6bd375a2f709d3f21ac2d8eb13e381a0d5cf2ea247c1269\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-337h43p3/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=8c5aea63e8e4afce2e86ec685b1b24a02bb691a2c36eb81c5626ee458aebde8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl (164.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Collecting torch==2.3.1 (from xformers)\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->xformers) (12.1.105)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1->xformers)\n",
            "  Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->xformers) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1->xformers) (1.3.0)\n",
            "Installing collected packages: triton, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.3.1 triton-2.3.1 xformers-0.0.27\n",
            "/content\n",
            "--2024-07-17 08:47:17--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.206.154, 164.52.210.97, 164.52.210.96, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.206.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4811880516 (4.5G) [application/zip]\n",
            "Saving to: ‘en2indic.zip’\n",
            "\n",
            "en2indic.zip        100%[===================>]   4.48G  9.78MB/s    in 7m 8s   \n",
            "\n",
            "2024-07-17 08:54:27 (10.7 MB/s) - ‘en2indic.zip’ saved [4811880516/4811880516]\n",
            "\n",
            "Archive:  en2indic.zip\n",
            "   creating: en-indic/\n",
            "   creating: en-indic/vocab/\n",
            "  inflating: en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: en-indic/vocab/vocab.SRC  \n",
            "  inflating: en-indic/vocab/vocab.TGT  \n",
            "  inflating: en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: en-indic/final_bin/\n",
            "  inflating: en-indic/final_bin/preprocess.log  \n",
            "  inflating: en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: en-indic/model/\n",
            "  inflating: en-indic/model/checkpoint_best.pt  \n",
            "/content/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd ..\n",
        "# Install requirements\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# xformers\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd ..\n",
        "# download the en2indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "!unzip en2indic.zip\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "from indicTrans.inference.engine import Model\n",
        "en2indic_model = Model(expdir='../en-indic')"
      ],
      "metadata": {
        "id": "lySf-D4mekvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738ea5ba-ea09-4384-b938-0f4f03ebe40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:Triton is not available, some optimizations will not be enabled.\n",
            "This is just a warning: triton is not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ79obB74oL8"
      },
      "outputs": [],
      "source": [
        "def translate_text(text, dest_language='kn'):  # Change 'kn' to your desired language code\n",
        "    try:\n",
        "        translated = en2indic_model.translate_paragraph(text, 'en', dest_language)\n",
        "        print(\"Translated:\", translated)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"Error in translation: {e}\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_newlines(text, translated_text):\n",
        "    # Find positions of '\\n' in the original text\n",
        "    newline_positions = [pos for pos, char in enumerate(text) if char == '\\n']\n",
        "\n",
        "    # Adjust positions for translated text\n",
        "    adjusted_positions = []\n",
        "    offset = 0\n",
        "    for pos in newline_positions:\n",
        "        while pos + offset < len(translated_text) and translated_text[pos + offset] != ' ':\n",
        "            offset += 1\n",
        "        adjusted_positions.append(pos + offset)\n",
        "\n",
        "    # Insert '\\n' at the adjusted positions\n",
        "    for pos in reversed(adjusted_positions):  # reversed to avoid messing up positions\n",
        "        translated_text = translated_text[:pos] + '\\n' + translated_text[pos:]\n",
        "\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "fiGLSbY6ZuPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the output JSON file\n",
        "output_json_path = r\"/content/extracted_text_with_coordinates.json\"\n",
        "\n",
        "# Dictionary to store text with coordinates and character count\n",
        "extracted_data = {}\n",
        "\n",
        "# Create a document object\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract the number of pages\n",
        "#print(f\"Number of pages: {doc.page_count}\")\n",
        "\n",
        "# Extract metadata\n",
        "#print(\"Metadata:\", doc.metadata)\n",
        "\n",
        "# Iterate through all pages\n",
        "for i in range(doc.page_count):\n",
        "    # Get the page\n",
        "    page = doc.load_page(i)  # or page = doc[i]\n",
        "\n",
        "    # Extract text blocks from the page\n",
        "    blocks = page.get_text(\"dict\", flags=11)[\"blocks\"]\n",
        "\n",
        "    blocks1 = page.get_text(\"blocks\")\n",
        "\n",
        "    # List to store text and coordinates for the current page\n",
        "    page_data = []\n",
        "\n",
        "\n",
        "\n",
        "    for b in blocks:\n",
        "        bbox = b[\"bbox\"]\n",
        "        for l in b[\"lines\"]:  # iterate through the text lines\n",
        "            for s in l[\"spans\"]:  # iterate through the text spans\n",
        "                #text = s[\"text\"]\n",
        "                #origin = s[\"origin\"]\n",
        "                font_size = s[\"size\"]\n",
        "                #character_count = len(text)\n",
        "\n",
        "        # Append to the result\n",
        "        page_data.append({\n",
        "            \"coordinates\": bbox,\n",
        "            #\"text\": text,\n",
        "            #\"IniCharacter_count\": character_count,\n",
        "            \"IniFontsize\": font_size,\n",
        "        })\n",
        "\n",
        "    # Add the page data to the extracted data dictionary\n",
        "    extracted_data[i+1] = page_data\n",
        "\n",
        "# Save the extracted data to a JSON file\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(extracted_data, json_file, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "jl3zqt2GXEjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the output JSON file\n",
        "json_path = r\"/content/extracted_text_with_coordinates.json\""
      ],
      "metadata": {
        "id": "JXTHHnn61E2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYFCel1m4uLr",
        "outputId": "85ad91d7-f24e-4ec5-9184-c9ae1f67c88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 261.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: 1 ಎನ್\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1207.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ರಾಧಾಕೃಷ್ಣನ್ ಪ್ರಶ್ನೆ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 1166.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ರಶೀದಾ ಪತ್ರಿಕೆ ಓದುತ್ತಾ ಕುಳಿತಿದ್ದಳು. ಇದ್ದಕ್ಕಿದ್ದಂತೆ, ಅವಳ ಕಣ್ಣುಗಳು ಒಂದು ಸಣ್ಣ ಶೀರ್ಷಿಕೆಯ ಮೇಲೆ ಬಂದವುಃ ನೂರು ವರ್ಷಗಳ ಹಿಂದೆ. “ ಇಷ್ಟು ವರ್ಷಗಳ ಹಿಂದೆ ಏನಾಯಿತು ಎಂದು ಯಾರಿಗಾದರೂ ಹೇಗೆ ಗೊತ್ತು? ”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1209.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ಏನಾಯಿತು ಎಂಬುದನ್ನು ಕಂಡುಹಿಡಿಯುವುದು\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 3057.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ನಿನ್ನೆ: ನೀವು ರೇಡಿಯೊವನ್ನು ಕೇಳಬಹುದು, ಟಿವಿ ನೋಡಬಹುದು, ಪತ್ರಿಕೆಯನ್ನು ಓದಬಹುದು. ಕಳೆದ ವರ್ಷಃ ನೆನಪಿಡುವ ಯಾರನ್ನಾದರೂ ಕೇಳಿ. ಆದರೆ ಬಹಳ ಹಿಂದೆಯೇ ಏನಾಯಿತು? ಇದನ್ನು ಹೇಗೆ ಮಾಡಬಹುದೆಂದು ನೋಡೋಣ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1746.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ನಾವು ಹಿಂದಿನ ಬಗ್ಗೆ ಏನು ತಿಳಿಯಬಹುದು?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 1404.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ಜನರು ಏನು ತಿನ್ನುತ್ತಾರೆ, ಅವರು ಯಾವ ರೀತಿಯ ಬಟ್ಟೆಗಳನ್ನು ಧರಿಸುತ್ತಾರೆ, ಅವರು ವಾಸಿಸುತ್ತಿದ್ದ ಮನೆಗಳಿವೆ ಎಂದು ನಾವು ಕಂಡುಹಿಡಿಯಬಹುದಾದ ಹಲವಾರು ವಿಷಯಗಳಿವೆ. ನಾವು ಬೇಟೆಗಾರರು, ಪಶುಪಾಲಕರು, ರೈತರು, ಆಡಳಿತಗಾರರು, ವ್ಯಾಪಾರಿಗಳು, ಪುರೋಹಿತರು, ಕರಕುಶಲಕರ್ಮಿಗಳು, ಕಲಾವಿದರು, ಸಂಗೀತಗಾರರು ಮತ್ತು ವಿಜ್ಞಾನಿಗಳ ಜೀವನದ ಬಗ್ಗೆ ತಿಳಿದುಕೊಳ್ಳಬಹುದು. ಮಕ್ಕಳು ಆಡಿದ ಆಟಗಳು, ಅವರು ಕೇಳಿದ ಕಥೆಗಳು, ಅವರು ನೋಡಿದ ನಾಟಕಗಳು, ಅವರು ಹಾಡಿದ ಹಾಡುಗಳ ಬಗ್ಗೆಯೂ ನಾವು ತಿಳಿದುಕೊಳ್ಳಬಹುದು.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1736.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ಜನರು ಎಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದರು?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 1603.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ನಕ್ಷೆ 1 ರಲ್ಲಿ ನರ್ಮದಾ ನದಿಯನ್ನು ಹುಡುಕಿ (ಪುಟ 2). ನೂರಾರು ವರ್ಷಗಳಿಂದ ಜನರು ಈ ನದಿಯ ದಡದಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದಾರೆ. ಇಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದ ಕೆಲವು ಆರಂಭಿಕ ಜನರು ಕುಶಲ ಸಂಗ್ರಹಿಸುವವರಾಗಿದ್ದರು, ಅಂದರೆ ತಮ್ಮ ಆಹಾರವನ್ನು ಸಂಗ್ರಹಿಸಿದ ಜನರು. ಸುತ್ತಮುತ್ತಲಿನ ಕಾಡುಗಳಲ್ಲಿನ ಸಸ್ಯಗಳ ಅಗಾಧ ಸಂಪತ್ತಿನ ಬಗ್ಗೆ ಅವರಿಗೆ ತಿಳಿದಿತ್ತು, ಮತ್ತು ತಮ್ಮ ಆಹಾರಕ್ಕಾಗಿ ಬೇರುಗಳು, ಹಣ್ಣುಗಳು ಮತ್ತು ಇತರ ಅರಣ್ಯ ಉತ್ಪನ್ನಗಳನ್ನು ಸಂಗ್ರಹಿಸಿದರು. ಅವರು ಪ್ರಾಣಿಗಳನ್ನೂ ಬೇಟೆಯಾಡುತ್ತಿದ್ದರು.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1548.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ಅಧ್ಯಾಯ 1 ಏನು, ಎಲ್ಲಿ, ಹೇಗೆ ಮತ್ತು ಯಾವಾಗ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 546.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: ಏನು, ಎಲ್ಲಿ, ಹೇಗೆ ಮತ್ತು ಯಾವಾಗ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1222.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: 2018-19 ನೇ ಸಾಲಿನಲ್ಲಿ ಶೇ.\n"
          ]
        }
      ],
      "source": [
        "# Load the JSON data\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    extracted_data = json.load(json_file)\n",
        "\n",
        "# Create a document object\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Iterate through the extracted data and translate the text\n",
        "for page_num, page_data in extracted_data.items():\n",
        "\n",
        "    # Get the page\n",
        "    page = doc.load_page(int(page_num) - 1)  # or page = doc[i]\n",
        "    blocks = page.get_text(\"blocks\")\n",
        "    for b,block in zip(blocks,page_data):\n",
        "        text = b[4]\n",
        "        block[\"text\"] = text\n",
        "        block[\"Character_count\"] = len(text)\n",
        "        translated_text = translate_text(text)  # Change 'kn' to your desired language code\n",
        "        block[\"Translated_text\"] = insert_newlines(block[\"text\"], translated_text)\n",
        "        block[\"translated_character_count\"] = len(translated_text)\n",
        "        f = block[\"IniFontsize\"]\n",
        "        if len(text) < len(translated_text):\n",
        "            block[\"Font_Size\"] = len(translated_text) * f / len(text)\n",
        "\n",
        "\n",
        "# Save the extracted data to a JSON file\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(extracted_data, json_file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGOzjbcn4IVo"
      },
      "outputs": [],
      "source": [
        "noto_sans_kannada_path = r\"/content/NotoSansKannada-VariableFont_wdth,wght.ttf\"\n",
        "if not os.path.isfile(noto_sans_kannada_path):\n",
        "    raise FileNotFoundError(f\"The font file was not found: {noto_sans_kannada_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    extracted_data = json.load(json_file)\n",
        "\n",
        "# Create a new PDF document\n",
        "new_doc = fitz.open(new_pdf_path)\n",
        "\n",
        "# Iterate through the pages in the JSON data\n",
        "for page_num, page_data in extracted_data.items():\n",
        "    # Load a page\n",
        "    new_page = new_doc.load_page(int(page_num)-1)\n",
        "\n",
        "    # Iterate through the text blocks on the current page\n",
        "    for block in page_data:\n",
        "        translated_text = block.get(\"Translated_text\", \"\")\n",
        "        coordinates = block[\"coordinates\"]\n",
        "        #origin = block[\"origin\"]\n",
        "        x0, y0, x1, y1 = coordinates[0], coordinates[1], coordinates[2], coordinates[3]\n",
        "\n",
        "        # Set the font size based on the height of the bounding box\n",
        "        font_size = block[\"IniFontsize\"]\n",
        "\n",
        "        # Draw translated text on the new page with the font file using insert_textbox\n",
        "        new_page.insert_text(\n",
        "            (x0,y0),\n",
        "            text=translated_text,\n",
        "            fontsize=font_size*0.75,\n",
        "            fontname='NotoSansKannada',\n",
        "            fontfile=noto_sans_kannada_path\n",
        "        )\n",
        "\n",
        "# Save the new PDF\n",
        "new_doc.saveIncr()\n",
        "\n",
        "# Close the document\n",
        "new_doc.close()"
      ],
      "metadata": {
        "id": "_LzxnZj9zfHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94XwmSkr5C2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "f51942ec-7d4f-4669-a346-6c25cafba6b9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "document closed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e0884f3d4c68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;34m\"\"\"Close document.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;31m# self._cleanup()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_outline\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: document closed"
          ]
        }
      ],
      "source": [
        "doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcMK87eqOyGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944890f4-551d-409a-e1a8-2de2becdeb0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: convertapi in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: requests>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from convertapi) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4.2->convertapi) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install convertapi\n",
        "import convertapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujb6R9GPOtJu"
      },
      "outputs": [],
      "source": [
        "convertapi.api_secret = '4OYbALQ2RREClhvm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBkMA7hQ14En"
      },
      "outputs": [],
      "source": [
        "docf = '/content/Translated_OurPast_Chapter1' #-----------------> new_pdf_path ***but without extension***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqkDYe4r5HQn",
        "outputId": "9a1cab66-8197-451d-c9d9-c0c40524b9a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Translated_OurPast_Chapter1.docx']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "convertapi.convert('docx', {\n",
        "    'File': f'{docf}.pdf'\n",
        "}, from_format = 'pdf').save_files('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X2CLoFKRBLT",
        "outputId": "d9d37401-45bf-46c2-9d46-ddf15a52c891"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Translated_OurPast_Chapter1.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "convertapi.convert('pdf', {\n",
        "    'File': f'{docf}.docx'\n",
        "}, from_format = 'docx').save_files('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnFC7a9SYNP",
        "outputId": "b571caf7-5d69-45d0-c7f7-e2836eafa370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New PDF and Docx with translated content generated successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"New PDF and Docx with translated content generated successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}